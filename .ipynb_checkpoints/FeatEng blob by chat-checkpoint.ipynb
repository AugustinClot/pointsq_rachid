{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d17c89-3a56-44a4-9169-caffeb0576ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering Cookbook for 6-hour dataset interviews (Time Series / Finance)\n",
    "# Paste into Jupyter. Minimal assumptions: you have a pandas DataFrame `df` with a DatetimeIndex.\n",
    "# If you have a timestamp column instead, see section 0.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------\n",
    "# 0) Setup / Assumptions\n",
    "# -------------------------\n",
    "# Assume df has columns like: ['open','high','low','close','volume', ...]\n",
    "# and df.index is a DatetimeIndex sorted ascending.\n",
    "# If you have a timestamp column:\n",
    "# df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "# df = df.sort_values('timestamp').set_index('timestamp')\n",
    "\n",
    "def ensure_datetime_index(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"df.index must be a DatetimeIndex. Convert via pd.to_datetime + set_index.\")\n",
    "    return df.sort_index()\n",
    "\n",
    "df = ensure_datetime_index(df)\n",
    "\n",
    "# Optional: pick a primary series\n",
    "PRICE_COL = 'close' if 'close' in df.columns else df.columns[0]\n",
    "RET_COL = 'logret_1'\n",
    "\n",
    "# -------------------------\n",
    "# 1) Differencing & Returns\n",
    "# -------------------------\n",
    "# 1.1 Arithmetic differences\n",
    "df['diff_1'] = df[PRICE_COL].diff(1)\n",
    "df['diff_5'] = df[PRICE_COL].diff(5)\n",
    "\n",
    "# 1.2 Log returns (preferred for prices > 0)\n",
    "df[RET_COL] = np.log(df[PRICE_COL]).diff(1)\n",
    "\n",
    "# 1.3 Multi-step returns\n",
    "for k in [2, 5, 10, 20]:\n",
    "    df[f'logret_{k}'] = np.log(df[PRICE_COL]).diff(k)\n",
    "\n",
    "# 1.4 Vol-adjusted return (rolling z-score of returns)\n",
    "w = 20\n",
    "df['ret_mu_20'] = df[RET_COL].rolling(w).mean()\n",
    "df['ret_sigma_20'] = df[RET_COL].rolling(w).std(ddof=0)\n",
    "df['ret_z_20'] = (df[RET_COL] - df['ret_mu_20']) / df['ret_sigma_20']\n",
    "\n",
    "# -------------------------\n",
    "# 2) Lag Features\n",
    "# -------------------------\n",
    "# Lags for returns / target-like series (choose a few meaningful lags)\n",
    "LAGS = [1, 2, 3, 5, 10, 20]\n",
    "for lag in LAGS:\n",
    "    df[f'{RET_COL}_lag{lag}'] = df[RET_COL].shift(lag)\n",
    "    df[f'{PRICE_COL}_lag{lag}'] = df[PRICE_COL].shift(lag)\n",
    "\n",
    "# If you have additional predictors:\n",
    "# for col in ['open','high','low','volume']:\n",
    "#     for lag in [1,5,20]:\n",
    "#         df[f'{col}_lag{lag}'] = df[col].shift(lag)\n",
    "\n",
    "# -------------------------\n",
    "# 3) Rolling Window Statistics\n",
    "# -------------------------\n",
    "# Rolling stats on returns and/or price\n",
    "WINDOWS = [5, 10, 20, 60]\n",
    "for w in WINDOWS:\n",
    "    # on returns (volatility etc.)\n",
    "    df[f'{RET_COL}_roll_mean_{w}'] = df[RET_COL].rolling(w).mean()\n",
    "    df[f'{RET_COL}_roll_std_{w}']  = df[RET_COL].rolling(w).std(ddof=0)\n",
    "    df[f'{RET_COL}_roll_min_{w}']  = df[RET_COL].rolling(w).min()\n",
    "    df[f'{RET_COL}_roll_max_{w}']  = df[RET_COL].rolling(w).max()\n",
    "    df[f'{RET_COL}_roll_q10_{w}']  = df[RET_COL].rolling(w).quantile(0.10)\n",
    "    df[f'{RET_COL}_roll_q90_{w}']  = df[RET_COL].rolling(w).quantile(0.90)\n",
    "    # z-score of return vs rolling window\n",
    "    mu = df[f'{RET_COL}_roll_mean_{w}']\n",
    "    sd = df[f'{RET_COL}_roll_std_{w}']\n",
    "    df[f'{RET_COL}_roll_z_{w}'] = (df[RET_COL] - mu) / sd\n",
    "\n",
    "    # on price (trend proxy)\n",
    "    df[f'{PRICE_COL}_roll_mean_{w}'] = df[PRICE_COL].rolling(w).mean()\n",
    "    df[f'{PRICE_COL}_roll_std_{w}']  = df[PRICE_COL].rolling(w).std(ddof=0)\n",
    "    df[f'{PRICE_COL}_roll_min_{w}']  = df[PRICE_COL].rolling(w).min()\n",
    "    df[f'{PRICE_COL}_roll_max_{w}']  = df[PRICE_COL].rolling(w).max()\n",
    "\n",
    "# -------------------------\n",
    "# 4) Calendar Features + Cyclical Encoding\n",
    "# -------------------------\n",
    "# Raw calendar features\n",
    "df['hour']      = df.index.hour\n",
    "df['dayofweek'] = df.index.dayofweek\n",
    "df['dayofmonth']= df.index.day\n",
    "df['dayofyear'] = df.index.dayofyear\n",
    "df['weekofyear']= df.index.isocalendar().week.astype(int)\n",
    "df['month']     = df.index.month\n",
    "df['quarter']   = df.index.quarter\n",
    "df['year']      = df.index.year\n",
    "\n",
    "# Cyclical encoding helper\n",
    "def add_cyclical(df: pd.DataFrame, col: str, period: int) -> None:\n",
    "    angle = 2.0 * np.pi * df[col] / period\n",
    "    df[f'{col}_sin'] = np.sin(angle)\n",
    "    df[f'{col}_cos'] = np.cos(angle)\n",
    "\n",
    "# Apply cyclical transforms\n",
    "# Note: dayofmonth period varies; use 31 as approximation or skip.\n",
    "add_cyclical(df, 'hour', 24)\n",
    "add_cyclical(df, 'dayofweek', 7)\n",
    "add_cyclical(df, 'month', 12)\n",
    "add_cyclical(df, 'weekofyear', 52)\n",
    "\n",
    "# -------------------------\n",
    "# 5) Trend / Momentum / \"Technical\" Indicators (simple implementations)\n",
    "# -------------------------\n",
    "# 5.1 SMA / EMA\n",
    "for w in [10, 20, 50]:\n",
    "    df[f'sma_{w}'] = df[PRICE_COL].rolling(w).mean()\n",
    "    df[f'ema_{w}'] = df[PRICE_COL].ewm(span=w, adjust=False).mean()\n",
    "    # price relative to MA\n",
    "    df[f'price_minus_sma_{w}'] = df[PRICE_COL] - df[f'sma_{w}']\n",
    "    df[f'price_over_sma_{w}']  = df[PRICE_COL] / df[f'sma_{w}']\n",
    "\n",
    "# 5.2 MACD (12-26 EMA + signal 9)\n",
    "ema12 = df[PRICE_COL].ewm(span=12, adjust=False).mean()\n",
    "ema26 = df[PRICE_COL].ewm(span=26, adjust=False).mean()\n",
    "df['macd'] = ema12 - ema26\n",
    "df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()\n",
    "df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "\n",
    "# 5.3 RSI (14)\n",
    "def rsi(series: pd.Series, period: int = 14) -> pd.Series:\n",
    "    delta = series.diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -delta.clip(upper=0)\n",
    "    avg_gain = gain.ewm(alpha=1/period, adjust=False).mean()\n",
    "    avg_loss = loss.ewm(alpha=1/period, adjust=False).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "df['rsi_14'] = rsi(df[PRICE_COL], 14)\n",
    "\n",
    "# 5.4 Rate of Change (ROC)\n",
    "for k in [5, 10, 20]:\n",
    "    df[f'roc_{k}'] = df[PRICE_COL].pct_change(k)\n",
    "\n",
    "# -------------------------\n",
    "# 6) Volatility & Range / OHLC-based features\n",
    "# -------------------------\n",
    "if set(['open','high','low','close']).issubset(df.columns):\n",
    "    # Intrabar range features\n",
    "    df['hl_range'] = df['high'] - df['low']\n",
    "    df['oc_change'] = df['close'] - df['open']\n",
    "    df['hl_over_close'] = (df['high'] - df['low']) / df['close']\n",
    "    df['body_over_range'] = (df['close'] - df['open']).abs() / (df['high'] - df['low']).replace(0, np.nan)\n",
    "\n",
    "    # Parkinson volatility (uses high/low, more efficient estimator under assumptions)\n",
    "    # sigma^2 â‰ˆ (1/(4 ln 2)) * (ln(H/L))^2\n",
    "    hl_log = np.log(df['high'] / df['low'])\n",
    "    df['parkinson_var'] = (hl_log ** 2) / (4 * np.log(2))\n",
    "    for w in [10, 20]:\n",
    "        df[f'parkinson_vol_{w}'] = np.sqrt(df['parkinson_var'].rolling(w).mean())\n",
    "\n",
    "# Realized volatility from returns\n",
    "for w in [10, 20, 60]:\n",
    "    df[f'realized_vol_{w}'] = df[RET_COL].rolling(w).std(ddof=0)\n",
    "\n",
    "# Volatility regime flags\n",
    "df['vol_z_60'] = (df['realized_vol_20'] - df['realized_vol_20'].rolling(60).mean()) / df['realized_vol_20'].rolling(60).std(ddof=0)\n",
    "df['high_vol_flag'] = (df['vol_z_60'] > 1.0).astype(int)\n",
    "\n",
    "# -------------------------\n",
    "# 7) Distributional Shape Features (skew/kurtosis, drawdown, tails)\n",
    "# -------------------------\n",
    "# Rolling skew/kurtosis (pandas uses sample estimators; fine for features)\n",
    "for w in [20, 60]:\n",
    "    df[f'ret_skew_{w}'] = df[RET_COL].rolling(w).skew()\n",
    "    df[f'ret_kurt_{w}'] = df[RET_COL].rolling(w).kurt()\n",
    "\n",
    "# Rolling max drawdown (on price)\n",
    "def rolling_max_drawdown(price: pd.Series, window: int) -> pd.Series:\n",
    "    roll_max = price.rolling(window).max()\n",
    "    dd = price / roll_max - 1.0\n",
    "    return dd.rolling(window).min()  # most negative within window\n",
    "\n",
    "df['mdd_60'] = rolling_max_drawdown(df[PRICE_COL], 60)\n",
    "\n",
    "# Tail quantiles of returns\n",
    "for w in [20, 60]:\n",
    "    df[f'ret_q05_{w}'] = df[RET_COL].rolling(w).quantile(0.05)\n",
    "    df[f'ret_q95_{w}'] = df[RET_COL].rolling(w).quantile(0.95)\n",
    "\n",
    "# -------------------------\n",
    "# 8) Interaction Features (manual, lightweight)\n",
    "# -------------------------\n",
    "# Return x Vol, Momentum x Regime, etc.\n",
    "df['ret_x_vol20'] = df[RET_COL] * df['realized_vol_20']\n",
    "df['macd_x_highvol'] = df['macd'] * df['high_vol_flag'] if 'macd' in df.columns else np.nan\n",
    "df['rsi_x_highvol'] = df['rsi_14'] * df['high_vol_flag'] if 'rsi_14' in df.columns else np.nan\n",
    "\n",
    "# -------------------------\n",
    "# 9) Change Detection / Regime-ish flags (simple)\n",
    "# -------------------------\n",
    "# Large move flags\n",
    "df['abs_ret'] = df[RET_COL].abs()\n",
    "df['big_move_flag'] = (df['abs_ret'] > df['abs_ret'].rolling(250).quantile(0.95)).astype(int)\n",
    "\n",
    "# Volatility jump flag (ratio)\n",
    "df['vol_ratio_20_60'] = df['realized_vol_20'] / df['realized_vol_60']\n",
    "df['vol_jump_flag'] = (df['vol_ratio_20_60'] > 1.5).astype(int)\n",
    "\n",
    "# -------------------------\n",
    "# 10) Missingness as Signal + Simple Imputation (only if needed)\n",
    "# -------------------------\n",
    "# Missingness indicators\n",
    "for col in df.columns:\n",
    "    if df[col].isna().any():\n",
    "        df[f'{col}_isna'] = df[col].isna().astype(int)\n",
    "\n",
    "# Example: forward-fill a feature column (be careful in finance; justify!)\n",
    "# df['volume_ffill'] = df['volume'].ffill()\n",
    "\n",
    "# -------------------------\n",
    "# 11) Cross-sectional / Relative features (only if you have multiple assets)\n",
    "# -------------------------\n",
    "# If your df has columns: ['asset_id', 'close', ...] and a timestamp index,\n",
    "# you typically do groupby features. Example:\n",
    "#\n",
    "# df = df.reset_index()  # timestamp\n",
    "# df = df.sort_values(['timestamp','asset_id'])\n",
    "# df['logret_1'] = df.groupby('asset_id')['close'].apply(lambda s: np.log(s).diff(1))\n",
    "#\n",
    "# # Rank within timestamp (cross-sectional)\n",
    "# df['ret_rank'] = df.groupby('timestamp')['logret_1'].rank(pct=True)\n",
    "#\n",
    "# # Z-score within timestamp\n",
    "# def cs_z(x):\n",
    "#     return (x - x.mean()) / x.std(ddof=0)\n",
    "# df['ret_cs_z'] = df.groupby('timestamp')['logret_1'].transform(cs_z)\n",
    "#\n",
    "# # Spread to cross-sectional median\n",
    "# df['ret_minus_median'] = df['logret_1'] - df.groupby('timestamp')['logret_1'].transform('median')\n",
    "#\n",
    "# df = df.set_index('timestamp')\n",
    "\n",
    "# -------------------------\n",
    "# 12) Categorical encoding (if you have categoricals)\n",
    "# -------------------------\n",
    "# 12.1 One-hot\n",
    "# if 'sector' in df.columns:\n",
    "#     df = df.join(pd.get_dummies(df['sector'], prefix='sector', drop_first=True))\n",
    "\n",
    "# 12.2 Frequency encoding (safe-ish, no target)\n",
    "def frequency_encode(s: pd.Series) -> pd.Series:\n",
    "    freq = s.value_counts(dropna=False)\n",
    "    return s.map(freq) / len(s)\n",
    "\n",
    "# Example:\n",
    "# df['sector_freq'] = frequency_encode(df['sector'])\n",
    "\n",
    "# 12.3 Target encoding (MUST be done with CV on training only; example function below)\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def target_encode_cv(X: pd.DataFrame, cat_col: str, y: pd.Series, n_splits: int = 5, seed: int = 42) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Leakage-aware target encoding using KFold.\n",
    "    Returns encoded values aligned with X.index.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    enc = pd.Series(index=X.index, dtype=float)\n",
    "    global_mean = y.mean()\n",
    "    for tr_idx, va_idx in kf.split(X):\n",
    "        tr = X.iloc[tr_idx]\n",
    "        va = X.iloc[va_idx]\n",
    "        means = y.iloc[tr_idx].groupby(tr[cat_col]).mean()\n",
    "        enc.iloc[va_idx] = va[cat_col].map(means).fillna(global_mean)\n",
    "    return enc\n",
    "\n",
    "# Example usage (ONLY for training):\n",
    "# df_train['sector_te'] = target_encode_cv(df_train, 'sector', y_train)\n",
    "\n",
    "# -------------------------\n",
    "# 13) Train/Test split + leakage-safe pipeline tips\n",
    "# -------------------------\n",
    "# Common: time-based split\n",
    "# split_point = int(len(df) * 0.8)\n",
    "# train = df.iloc[:split_point].copy()\n",
    "# test  = df.iloc[split_point:].copy()\n",
    "\n",
    "# When using sklearn Pipelines: create features BEFORE split or inside a custom transformer that only uses past.\n",
    "# For rolling features, it's generally OK to compute on full df as long as you use only past values;\n",
    "# but if you do normalization/scaling, fit scaler ONLY on train.\n",
    "\n",
    "# -------------------------\n",
    "# 14) Final: Assemble feature matrix\n",
    "# -------------------------\n",
    "# Example: choose features excluding raw target\n",
    "TARGET = RET_COL  # or your label column\n",
    "exclude = {TARGET}\n",
    "FEATURES = [c for c in df.columns if c not in exclude]\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df[TARGET]\n",
    "\n",
    "# Clean NaNs from lag/rolling creation (standard)\n",
    "data = pd.concat([X, y], axis=1).dropna()\n",
    "X_clean = data[FEATURES]\n",
    "y_clean = data[TARGET]\n",
    "\n",
    "print(\"X_clean shape:\", X_clean.shape, \"y_clean shape:\", y_clean.shape)\n",
    "print(\"Example feature columns:\", FEATURES[:20])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
