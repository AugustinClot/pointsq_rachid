{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Notebook\n",
    "The goal of this notebook is to do all of the data analysis / feature engineering part. \n",
    "This notebook does not train models, it focuses exclusively on feature creation and target definition\n",
    "At the end, we save the prepared data so that it can be used in the model's notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Setup & Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, classification_report, precision_recall_curve\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (17499636, 19)\n",
      "Test shape: (4393179, 19)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train = pd.read_parquet('train.parquet')\n",
    "test = pd.read_parquet('test.parquet')\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total users: 19,140\n",
      "Churned users: 4,271\n",
      "Churn rate: 22.31%\n",
      "\n",
      "Train users: 19,140\n",
      "Test users: 2,904\n"
     ]
    }
   ],
   "source": [
    "# Basic churn stats\n",
    "churned_users = train[train[\"page\"] == \"Cancellation Confirmation\"][\"userId\"].unique()\n",
    "\n",
    "total_users = int(train[\"userId\"].nunique())\n",
    "num_churned = len(churned_users)\n",
    "\n",
    "test_user_ids = test[\"userId\"].unique()\n",
    "\n",
    "print(f\"Total users: {total_users:,}\")\n",
    "print(f\"Churned users: {num_churned:,}\")\n",
    "print(f\"Churn rate: {num_churned / total_users:.2%}\")\n",
    "\n",
    "print(f\"\\nTrain users: {train['userId'].nunique():,}\")\n",
    "print(f\"Test users: {len(test_user_ids):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>gender</th>\n",
       "      <th>firstName</th>\n",
       "      <th>level</th>\n",
       "      <th>lastName</th>\n",
       "      <th>userId</th>\n",
       "      <th>ts</th>\n",
       "      <th>auth</th>\n",
       "      <th>page</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>location</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>method</th>\n",
       "      <th>length</th>\n",
       "      <th>song</th>\n",
       "      <th>artist</th>\n",
       "      <th>time</th>\n",
       "      <th>registration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>M</td>\n",
       "      <td>Shlok</td>\n",
       "      <td>paid</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>1749042</td>\n",
       "      <td>1538352001000</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>22683</td>\n",
       "      <td>Dallas-Fort Worth-Arlington, TX</td>\n",
       "      <td>278</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>PUT</td>\n",
       "      <td>524.32934</td>\n",
       "      <td>Ich mache einen Spiegel - Dream Part 4</td>\n",
       "      <td>Popol Vuh</td>\n",
       "      <td>2018-10-01 00:00:01</td>\n",
       "      <td>2018-08-08 13:22:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>200</td>\n",
       "      <td>M</td>\n",
       "      <td>Shlok</td>\n",
       "      <td>paid</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>1749042</td>\n",
       "      <td>1538352525000</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>22683</td>\n",
       "      <td>Dallas-Fort Worth-Arlington, TX</td>\n",
       "      <td>279</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>PUT</td>\n",
       "      <td>178.02404</td>\n",
       "      <td>Monster (Album Version)</td>\n",
       "      <td>Skillet</td>\n",
       "      <td>2018-10-01 00:08:45</td>\n",
       "      <td>2018-08-08 13:22:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>200</td>\n",
       "      <td>M</td>\n",
       "      <td>Shlok</td>\n",
       "      <td>paid</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>1749042</td>\n",
       "      <td>1538352703000</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>22683</td>\n",
       "      <td>Dallas-Fort Worth-Arlington, TX</td>\n",
       "      <td>280</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>PUT</td>\n",
       "      <td>232.61995</td>\n",
       "      <td>Seven Nation Army</td>\n",
       "      <td>The White Stripes</td>\n",
       "      <td>2018-10-01 00:11:43</td>\n",
       "      <td>2018-08-08 13:22:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>200</td>\n",
       "      <td>M</td>\n",
       "      <td>Shlok</td>\n",
       "      <td>paid</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>1749042</td>\n",
       "      <td>1538352935000</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>22683</td>\n",
       "      <td>Dallas-Fort Worth-Arlington, TX</td>\n",
       "      <td>281</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>PUT</td>\n",
       "      <td>265.50812</td>\n",
       "      <td>Under The Bridge (Album Version)</td>\n",
       "      <td>Red Hot Chili Peppers</td>\n",
       "      <td>2018-10-01 00:15:35</td>\n",
       "      <td>2018-08-08 13:22:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2366</th>\n",
       "      <td>200</td>\n",
       "      <td>M</td>\n",
       "      <td>Shlok</td>\n",
       "      <td>paid</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>1749042</td>\n",
       "      <td>1538353200000</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>22683</td>\n",
       "      <td>Dallas-Fort Worth-Arlington, TX</td>\n",
       "      <td>282</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>PUT</td>\n",
       "      <td>471.69261</td>\n",
       "      <td>Circlesong 6</td>\n",
       "      <td>Bobby McFerrin</td>\n",
       "      <td>2018-10-01 00:20:00</td>\n",
       "      <td>2018-08-08 13:22:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      status gender firstName level lastName   userId             ts  \\\n",
       "0        200      M     Shlok  paid  Johnson  1749042  1538352001000   \n",
       "992      200      M     Shlok  paid  Johnson  1749042  1538352525000   \n",
       "1360     200      M     Shlok  paid  Johnson  1749042  1538352703000   \n",
       "1825     200      M     Shlok  paid  Johnson  1749042  1538352935000   \n",
       "2366     200      M     Shlok  paid  Johnson  1749042  1538353200000   \n",
       "\n",
       "           auth      page  sessionId                         location  \\\n",
       "0     Logged In  NextSong      22683  Dallas-Fort Worth-Arlington, TX   \n",
       "992   Logged In  NextSong      22683  Dallas-Fort Worth-Arlington, TX   \n",
       "1360  Logged In  NextSong      22683  Dallas-Fort Worth-Arlington, TX   \n",
       "1825  Logged In  NextSong      22683  Dallas-Fort Worth-Arlington, TX   \n",
       "2366  Logged In  NextSong      22683  Dallas-Fort Worth-Arlington, TX   \n",
       "\n",
       "      itemInSession                                          userAgent method  \\\n",
       "0               278  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...    PUT   \n",
       "992             279  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...    PUT   \n",
       "1360            280  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...    PUT   \n",
       "1825            281  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...    PUT   \n",
       "2366            282  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...    PUT   \n",
       "\n",
       "         length                                    song  \\\n",
       "0     524.32934  Ich mache einen Spiegel - Dream Part 4   \n",
       "992   178.02404                 Monster (Album Version)   \n",
       "1360  232.61995                       Seven Nation Army   \n",
       "1825  265.50812        Under The Bridge (Album Version)   \n",
       "2366  471.69261                            Circlesong 6   \n",
       "\n",
       "                     artist                time        registration  \n",
       "0                 Popol Vuh 2018-10-01 00:00:01 2018-08-08 13:22:21  \n",
       "992                 Skillet 2018-10-01 00:08:45 2018-08-08 13:22:21  \n",
       "1360      The White Stripes 2018-10-01 00:11:43 2018-08-08 13:22:21  \n",
       "1825  Red Hot Chili Peppers 2018-10-01 00:15:35 2018-08-08 13:22:21  \n",
       "2366         Bobby McFerrin 2018-10-01 00:20:00 2018-08-08 13:22:21  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['status', 'gender', 'firstName', 'level', 'lastName', 'userId', 'ts', 'auth', 'page', 'sessionId', 'location', 'itemInSession', 'userAgent', 'method', 'length', 'song', 'artist', 'time', 'registration']\n",
      "status                    int64\n",
      "gender                   object\n",
      "firstName                object\n",
      "level                    object\n",
      "lastName                 object\n",
      "userId                   object\n",
      "ts                        int64\n",
      "auth                     object\n",
      "page                     object\n",
      "sessionId                 int64\n",
      "location                 object\n",
      "itemInSession             int64\n",
      "userAgent                object\n",
      "method                   object\n",
      "length                  float64\n",
      "song                     object\n",
      "artist                   object\n",
      "time             datetime64[us]\n",
      "registration     datetime64[us]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# View columns and sample data\n",
    "print(train.columns.tolist())\n",
    "print(train.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cutoff date : 2018-11-10 00:00:00\n",
      "Test cutoff date     : 2018-11-20 00:00:00\n",
      "Churn horizon (days) : 10\n",
      "\n",
      "Excluded pages:\n",
      "  - Cancellation Confirmation\n"
     ]
    }
   ],
   "source": [
    "# Time horizon for churn definition\n",
    "churn_horizon_days = 10\n",
    "\n",
    "# Snapshot dates used to construct features\n",
    "cutoff_train = pd.Timestamp('2018-11-10')\n",
    "cutoff_test = pd.Timestamp('2018-11-20')\n",
    "\n",
    "print(f\"Training cutoff date : {cutoff_train}\")\n",
    "print(f\"Test cutoff date     : {cutoff_test}\")\n",
    "print(f\"Churn horizon (days) : {churn_horizon_days}\")\n",
    "\n",
    "# Pages that must never be used as features\n",
    "# These directly encode churn outcomes or intent\n",
    "excluded_pages = {\n",
    "    #'Cancel',\n",
    "    'Cancellation Confirmation'\n",
    "    #'downgrade'? \n",
    "}\n",
    "\n",
    "print(\"\\nExcluded pages:\")\n",
    "for p in excluded_pages:\n",
    "    print(f\"  - {p}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1. Snapshot filtering\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def _filter_events(events, snapshot_date, allowed_users=None):\n",
    "    events_snapshot = events[events[\"time\"] <= snapshot_date].copy()\n",
    "    if allowed_users is not None:\n",
    "        events_snapshot = events_snapshot[events_snapshot[\"userId\"].isin(allowed_users)]\n",
    "    return events_snapshot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 2. Core aggregations\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def _add_core_aggregates(events_snapshot):\n",
    "    return (\n",
    "        events_snapshot.groupby(\"userId\")\n",
    "        .agg(\n",
    "            num_sessions=(\"sessionId\", \"nunique\"),\n",
    "            total_page_views=(\"page\", \"count\"),\n",
    "            songs_played=(\"song\", lambda x: x.notna().sum()),\n",
    "            avg_song_length=(\"length\", \"mean\"),\n",
    "            total_listening_time=(\"length\", \"sum\"),\n",
    "            std_song_length=(\"length\", \"std\"),\n",
    "            avg_items_per_session=(\"itemInSession\", \"mean\"),\n",
    "            max_items_per_session=(\"itemInSession\", \"max\"),\n",
    "            std_items_per_session=(\"itemInSession\", \"std\"),\n",
    "            first_activity=(\"time\", \"min\"),\n",
    "            last_activity=(\"time\", \"max\"),\n",
    "            unique_artists=(\"artist\", \"nunique\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 3. Temporal + rate features\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def _add_temporal_features(features, events_snapshot, snapshot_date):\n",
    "    features[\"days_active\"] = (\n",
    "        features[\"last_activity\"] - features[\"first_activity\"]\n",
    "    ).dt.days + 1\n",
    "\n",
    "    registration_df = events_snapshot.groupby(\"userId\")[\"registration\"].first().reset_index()\n",
    "    features = features.merge(registration_df, on=\"userId\", how=\"left\")\n",
    "\n",
    "    features[\"days_since_registration\"] = (snapshot_date - features[\"registration\"]).dt.days\n",
    "    features[\"days_since_last_activity\"] = (snapshot_date - features[\"last_activity\"]).dt.days\n",
    "\n",
    "    features[\"pages_per_day\"] = features[\"total_page_views\"] / features[\"days_active\"].clip(lower=1)\n",
    "    features[\"sessions_per_day\"] = features[\"num_sessions\"] / features[\"days_active\"].clip(lower=1)\n",
    "    features[\"songs_per_session\"] = features[\"songs_played\"] / features[\"num_sessions\"].clip(lower=1)\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 4. Page interaction counts (leakage-safe)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def _add_page_counts(features, events_snapshot):\n",
    "    # EXACT MATCH with fixed notebook\n",
    "    events_no_leak = events_snapshot.loc[\n",
    "        ~events_snapshot[\"page\"].isin(excluded_pages)\n",
    "    ].copy()\n",
    "\n",
    "    page_counts = (\n",
    "        events_no_leak\n",
    "        .groupby([\"userId\", \"page\"])\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "    )\n",
    "\n",
    "    page_counts.columns = [\n",
    "        \"page_\" + col.replace(\" \", \"_\") for col in page_counts.columns\n",
    "    ]\n",
    "\n",
    "    return features.merge(page_counts.reset_index(), on=\"userId\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 5. Subscription & demographics\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def _add_user_profile(features, events_snapshot):\n",
    "    level_df = (\n",
    "        events_snapshot.sort_values(\"time\")\n",
    "        .groupby(\"userId\")[\"level\"]\n",
    "        .agg([\"first\", \"last\"])\n",
    "        .reset_index()\n",
    "    )\n",
    "    level_df.columns = [\"userId\", \"first_level\", \"last_level\"]\n",
    "    features = features.merge(level_df, on=\"userId\", how=\"left\")\n",
    "\n",
    "    features[\"is_paid\"] = (features[\"last_level\"] == \"paid\").astype(int)\n",
    "    features[\"downgraded\"] = (\n",
    "        (features[\"first_level\"] == \"paid\") & (features[\"last_level\"] == \"free\")\n",
    "    ).astype(int)\n",
    "    features[\"upgraded\"] = (\n",
    "        (features[\"first_level\"] == \"free\") & (features[\"last_level\"] == \"paid\")\n",
    "    ).astype(int)\n",
    "\n",
    "    gender_df = events_snapshot.groupby(\"userId\")[\"gender\"].first().reset_index()\n",
    "    features = features.merge(gender_df, on=\"userId\", how=\"left\")\n",
    "    features[\"is_male\"] = (features[\"gender\"] == \"M\").astype(int)\n",
    "\n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 6. Recent activity windows + trends\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def _add_recent_activity(features, events_snapshot, snapshot_date):\n",
    "    for window in [3, 7, 14]:\n",
    "        recent = events_snapshot[events_snapshot[\"time\"] >= snapshot_date - timedelta(days=window)]\n",
    "        recent_agg = (\n",
    "            recent.groupby(\"userId\")\n",
    "            .agg(page=(\"page\", \"count\"), song=(\"song\", lambda x: x.notna().sum()))\n",
    "            .reset_index()\n",
    "        )\n",
    "        recent_agg.columns = [\"userId\", f\"recent_{window}d_pages\", f\"recent_{window}d_songs\"]\n",
    "        features = features.merge(recent_agg, on=\"userId\", how=\"left\")\n",
    "        features[f\"recent_{window}d_pages\"] = features[f\"recent_{window}d_pages\"].fillna(0)\n",
    "        features[f\"recent_{window}d_songs\"] = features[f\"recent_{window}d_songs\"].fillna(0)\n",
    "\n",
    "    features[\"activity_trend_7d\"] = (features[\"recent_7d_pages\"] / 7) / (features[\"pages_per_day\"] + 0.01)\n",
    "    features[\"activity_trend_3d\"] = (features[\"recent_3d_pages\"] / 3) / (features[\"pages_per_day\"] + 0.01)\n",
    "    features[\"recency_ratio\"] = (\n",
    "        features[\"days_since_last_activity\"] / features[\"days_since_registration\"].clip(lower=1)\n",
    "    )\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 7. Advanced engagement & disengagement features\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def _add_advanced_features(features, events_snapshot, snapshot_date):\n",
    "    last_song_df = (\n",
    "        events_snapshot[events_snapshot[\"song\"].notna()]\n",
    "        .groupby(\"userId\")[\"time\"]\n",
    "        .max()\n",
    "        .reset_index(name=\"last_song_time\")\n",
    "    )\n",
    "    features = features.merge(last_song_df, on=\"userId\", how=\"left\")\n",
    "\n",
    "    features[\"days_without_song\"] = (snapshot_date - features[\"last_song_time\"]).dt.days\n",
    "    features[\"days_without_song\"] = features[\"days_without_song\"].fillna(\n",
    "        features[\"days_since_registration\"]\n",
    "    )\n",
    "    features.drop(columns=[\"last_song_time\"], inplace=True)\n",
    "\n",
    "    features[\"error_rate\"] = features.get(\"page_Error\", 0) / (features[\"total_page_views\"] + 1)\n",
    "    features[\"help_rate\"] = features.get(\"page_Help\", 0) / (features[\"num_sessions\"] + 1)\n",
    "\n",
    "    features[\"activity_acceleration\"] = (\n",
    "        features[\"recent_3d_pages\"] / 3 - features[\"recent_7d_pages\"] / 7\n",
    "    )\n",
    "\n",
    "    old_pages = features[\"total_page_views\"] - features[\"recent_7d_pages\"]\n",
    "    features[\"recent_vs_old_ratio\"] = features[\"recent_7d_pages\"] / (old_pages + 1)\n",
    "\n",
    "    features[\"session_consistency\"] = 1 / (features[\"std_items_per_session\"] + 1)\n",
    "\n",
    "    interaction_cols = [\n",
    "        \"page_Thumbs_Up\",\n",
    "        \"page_Thumbs_Down\",\n",
    "        \"page_Add_Friend\",\n",
    "        \"page_Add_to_Playlist\",\n",
    "    ]\n",
    "    interaction_sum = sum(features[c] if c in features.columns else 0 for c in interaction_cols)\n",
    "    features[\"engagement_depth\"] = interaction_sum / (features[\"songs_played\"] + 1)\n",
    "\n",
    "    features[\"ad_rate\"] = features.get(\"page_Roll_Advert\", 0) / (features[\"num_sessions\"] + 1)\n",
    "    features[\"ad_per_song\"] = features.get(\"page_Roll_Advert\", 0) / (features[\"songs_played\"] + 1)\n",
    "\n",
    "    features[\"downgrade_intent\"] = (features.get(\"page_Downgrade\", 0) > 0).astype(int)\n",
    "\n",
    "    features[\"listening_intensity\"] = features[\"songs_played\"] / (features[\"days_active\"] * 24 + 1)\n",
    "    features[\"artist_diversity\"] = features[\"unique_artists\"] / (features[\"songs_played\"] + 1)\n",
    "\n",
    "    features[\"inactivity_pattern\"] = (\n",
    "        (features[\"recent_3d_pages\"] == 0).astype(int)\n",
    "        + (features[\"recent_7d_pages\"] < 5).astype(int)\n",
    "        + (features[\"recent_14d_pages\"] < 10).astype(int)\n",
    "    )\n",
    "    \n",
    "    # Thumbs ratios\n",
    "    if \"page_Thumbs_Up\" in features.columns and \"page_Thumbs_Down\" in features.columns:\n",
    "        features[\"thumbs_ratio\"] = (\n",
    "            features[\"page_Thumbs_Up\"]\n",
    "            / (features[\"page_Thumbs_Up\"] + features[\"page_Thumbs_Down\"] + 1)\n",
    "        )\n",
    "        features[\"total_thumbs\"] = (\n",
    "            features[\"page_Thumbs_Up\"] + features[\"page_Thumbs_Down\"]\n",
    "        )\n",
    "    \n",
    "    # Social engagement\n",
    "    if \"page_Add_Friend\" in features.columns:\n",
    "        features[\"friends_per_session\"] = (\n",
    "            features[\"page_Add_Friend\"] / features[\"num_sessions\"].clip(lower=1)\n",
    "        )\n",
    "    \n",
    "    if \"page_Add_to_Playlist\" in features.columns:\n",
    "        features[\"playlist_adds_per_session\"] = (\n",
    "            features[\"page_Add_to_Playlist\"] / features[\"num_sessions\"].clip(lower=1)\n",
    "        )\n",
    "\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 8. Time-of-week & hour-of-day features\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def _add_time_distribution(features, events_snapshot):\n",
    "    events_snapshot[\"dayofweek\"] = events_snapshot[\"time\"].dt.dayofweek\n",
    "\n",
    "    weekend = events_snapshot[events_snapshot[\"dayofweek\"] >= 5].groupby(\"userId\").size()\n",
    "    weekday = events_snapshot[events_snapshot[\"dayofweek\"] < 5].groupby(\"userId\").size()\n",
    "\n",
    "    features = features.merge(weekend.rename(\"weekend_pages\"), on=\"userId\", how=\"left\")\n",
    "    features = features.merge(weekday.rename(\"weekday_pages\"), on=\"userId\", how=\"left\")\n",
    "\n",
    "    features[\"weekend_pages\"] = features[\"weekend_pages\"].fillna(0)\n",
    "    features[\"weekday_pages\"] = features[\"weekday_pages\"].fillna(0)\n",
    "    features[\"weekend_ratio\"] = features[\"weekend_pages\"] / (features[\"weekday_pages\"] + 1)\n",
    "\n",
    "    events_snapshot[\"hour\"] = events_snapshot[\"time\"].dt.hour\n",
    "    peak = events_snapshot[(events_snapshot[\"hour\"] >= 18) & (events_snapshot[\"hour\"] <= 23)]\n",
    "    peak_pages = peak.groupby(\"userId\").size().rename(\"peak_hour_pages\")\n",
    "\n",
    "    features = features.merge(peak_pages, on=\"userId\", how=\"left\")\n",
    "    features[\"peak_hour_pages\"] = features[\"peak_hour_pages\"].fillna(0)\n",
    "    features[\"peak_hour_ratio\"] = features[\"peak_hour_pages\"] / (features[\"total_page_views\"] + 1)\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 9. Main orchestration function\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def build_user_features(events, snapshot_date, allowed_users=None):\n",
    "    print(f\"Creating features before {snapshot_date}...\")\n",
    "\n",
    "    events_snapshot = _filter_events(events, snapshot_date, allowed_users)\n",
    "\n",
    "    features = _add_core_aggregates(events_snapshot)\n",
    "    features = _add_temporal_features(features, events_snapshot, snapshot_date)\n",
    "    features = _add_page_counts(features, events_snapshot)\n",
    "    features = _add_user_profile(features, events_snapshot)\n",
    "    features = _add_recent_activity(features, events_snapshot, snapshot_date)\n",
    "    features = _add_advanced_features(features, events_snapshot, snapshot_date)\n",
    "    features = _add_time_distribution(features, events_snapshot)\n",
    "\n",
    "\n",
    "    # === EXACT MATCH CLEANUP ===\n",
    "    drop_cols = [\n",
    "        \"first_activity\",\n",
    "        \"last_activity\",\n",
    "        \"first_level\",\n",
    "        \"last_level\",\n",
    "        \"gender\",\n",
    "        \"registration\",\n",
    "    ]\n",
    "    features.drop(columns=drop_cols, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    leaky_cols = [\n",
    "        col for col in features.columns\n",
    "        if any(p.replace(\" \", \"_\") in col for p in excluded_pages)\n",
    "    ]\n",
    "    features.drop(columns=leaky_cols, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    return features.fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. Dataset Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â° Using training snapshot : 2018-11-10 00:00:00\n",
      "â° Using test snapshot     : 2018-11-20 00:00:00\n",
      "\n",
      "ðŸš¨ Churned users in window (2018-11-10 00:00:00 â†’ 2018-11-20 00:00:00): 667\n",
      "ðŸ‘¥ Users before snapshot: 18,880\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Training label construction\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Re-assert snapshot parameters (safety against cell re-execution)\n",
    "cutoff_train = pd.Timestamp(\"2018-11-10\")\n",
    "cutoff_test = pd.Timestamp(\"2018-11-20\")\n",
    "churn_horizon_days = 10\n",
    "\n",
    "print(f\"â° Using training snapshot : {cutoff_train}\")\n",
    "print(f\"â° Using test snapshot     : {cutoff_test}\")\n",
    "\n",
    "# Define churn observation window\n",
    "churn_window_end = cutoff_train + timedelta(days=churn_horizon_days)\n",
    "\n",
    "# Events occurring during the churn window\n",
    "churn_window_events = train[\n",
    "    (train[\"time\"] > cutoff_train) &\n",
    "    (train[\"time\"] <= churn_window_end)\n",
    "]\n",
    "\n",
    "# Users who churned within the window\n",
    "churned_user_ids = churn_window_events[\n",
    "    churn_window_events[\"page\"] == \"Cancellation Confirmation\"\n",
    "][\"userId\"].unique()\n",
    "\n",
    "# Users eligible for training (observed before snapshot)\n",
    "eligible_users = train[\n",
    "    train[\"time\"] <= cutoff_train\n",
    "][\"userId\"].unique()\n",
    "\n",
    "print(\n",
    "    f\"\\nðŸš¨ Churned users in window \"\n",
    "    f\"({cutoff_train} â†’ {churn_window_end}): {len(churned_user_ids)}\"\n",
    ")\n",
    "print(f\"ðŸ‘¥ Users before snapshot: {len(eligible_users):,}\")\n",
    "\n",
    "# Sanity check\n",
    "if len(churned_user_ids) == 0:\n",
    "    print(\"âš ï¸ WARNING: No churned users found! Check date ranges.\")\n",
    "    print(f\"   Train time range: {train['time'].min()} â†’ {train['time'].max()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features before 2018-11-10 00:00:00...\n",
      "\n",
      "ðŸŽ¯ Target distribution: {0: 18217, 1: 663}\n",
      "Churn rate: 3.51%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# Feature generation & target construction\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Build training features\n",
    "train_features = build_user_features(\n",
    "    train,\n",
    "    snapshot_date=cutoff_train,\n",
    "    allowed_users=eligible_users\n",
    ")\n",
    "\n",
    "# Ensure consistent userId type before label assignment\n",
    "train_features[\"userId\"] = train_features[\"userId\"].astype(str)\n",
    "churned_user_ids = [str(u) for u in churned_user_ids]\n",
    "\n",
    "# Create target label\n",
    "train_features[\"target\"] = train_features[\"userId\"].isin(churned_user_ids).astype(int)\n",
    "\n",
    "churn_rate = train_features[\"target\"].mean()\n",
    "print(f\"\\nðŸŽ¯ Target distribution: {train_features['target'].value_counts().to_dict()}\")\n",
    "print(f\"Churn rate: {churn_rate:.2%}\")\n",
    "\n",
    "if churn_rate == 0:\n",
    "    raise ValueError(\"âŒ Churn rate is 0%! Something is wrong with target creation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features before 2018-11-20 00:00:00...\n",
      "ðŸ“Š X_train: (18880, 72), X_test: (2904, 72)\n",
      "ðŸ“Š Number of features: 72\n",
      "\n",
      "ðŸ“‹ Feature list:\n",
      "  1. activity_acceleration\n",
      "  2. activity_trend_3d\n",
      "  3. activity_trend_7d\n",
      "  4. ad_per_song\n",
      "  5. ad_rate\n",
      "  6. artist_diversity\n",
      "  7. avg_items_per_session\n",
      "  8. avg_song_length\n",
      "  9. days_active\n",
      "  10. days_since_last_activity\n",
      "  11. days_since_registration\n",
      "  12. days_without_song\n",
      "  13. downgrade_intent\n",
      "  14. downgraded\n",
      "  15. engagement_depth\n",
      "  16. error_rate\n",
      "  17. friends_per_session\n",
      "  18. help_rate\n",
      "  19. inactivity_pattern\n",
      "  20. is_male\n",
      "  21. is_paid\n",
      "  22. listening_intensity\n",
      "  23. max_items_per_session\n",
      "  24. num_sessions\n",
      "  25. page_About\n",
      "  26. page_Add_Friend\n",
      "  27. page_Add_to_Playlist\n",
      "  28. page_Cancel\n",
      "  29. page_Downgrade\n",
      "  30. page_Error\n",
      "  31. page_Help\n",
      "  32. page_Home\n",
      "  33. page_Login\n",
      "  34. page_Logout\n",
      "  35. page_NextSong\n",
      "  36. page_Register\n",
      "  37. page_Roll_Advert\n",
      "  38. page_Save_Settings\n",
      "  39. page_Settings\n",
      "  40. page_Submit_Downgrade\n",
      "  41. page_Submit_Registration\n",
      "  42. page_Submit_Upgrade\n",
      "  43. page_Thumbs_Down\n",
      "  44. page_Thumbs_Up\n",
      "  45. page_Upgrade\n",
      "  46. pages_per_day\n",
      "  47. peak_hour_pages\n",
      "  48. peak_hour_ratio\n",
      "  49. playlist_adds_per_session\n",
      "  50. recency_ratio\n",
      "  51. recent_14d_pages\n",
      "  52. recent_14d_songs\n",
      "  53. recent_3d_pages\n",
      "  54. recent_3d_songs\n",
      "  55. recent_7d_pages\n",
      "  56. recent_7d_songs\n",
      "  57. recent_vs_old_ratio\n",
      "  58. session_consistency\n",
      "  59. sessions_per_day\n",
      "  60. songs_per_session\n",
      "  61. songs_played\n",
      "  62. std_items_per_session\n",
      "  63. std_song_length\n",
      "  64. thumbs_ratio\n",
      "  65. total_listening_time\n",
      "  66. total_page_views\n",
      "  67. total_thumbs\n",
      "  68. unique_artists\n",
      "  69. upgraded\n",
      "  70. weekday_pages\n",
      "  71. weekend_pages\n",
      "  72. weekend_ratio\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# Test feature generation\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "all_events = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "test_features = build_user_features(\n",
    "    all_events,\n",
    "    snapshot_date=cutoff_test,\n",
    "    allowed_users=test_user_ids\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Column alignment (train / test)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "feature_columns = [\n",
    "    c for c in train_features.columns if c not in [\"userId\", \"target\"]\n",
    "]\n",
    "\n",
    "all_feature_set = (set(feature_columns) | set(test_features.columns)) - {\"userId\", \"target\"}\n",
    "\n",
    "for col in all_feature_set:\n",
    "    if col not in train_features.columns:\n",
    "        train_features[col] = 0\n",
    "    if col not in test_features.columns:\n",
    "        test_features[col] = 0\n",
    "\n",
    "feature_columns = sorted(\n",
    "    c for c in train_features.columns if c not in [\"userId\", \"target\"]\n",
    ")\n",
    "\n",
    "X_train = train_features[feature_columns].fillna(0)\n",
    "y_train = train_features[\"target\"]\n",
    "X_test = test_features[feature_columns].fillna(0)\n",
    "\n",
    "print(f\"ðŸ“Š X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
    "print(f\"ðŸ“Š Number of features: {len(feature_columns)}\")\n",
    "\n",
    "print(\"\\nðŸ“‹ Feature list:\")\n",
    "for i, col in enumerate(feature_columns, start=1):\n",
    "    print(f\"  {i}. {col}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. Train / validation split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 15,104, Validation size: 3,776\n",
      "Train churn rate: 3.51%\n",
      "Validation churn rate: 3.52%\n"
     ]
    }
   ],
   "source": [
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Train size: {X_tr.shape[0]:,}, Validation size: {X_val.shape[0]:,}\")\n",
    "print(f\"Train churn rate: {y_tr.mean():.2%}\")\n",
    "print(f\"Validation churn rate: {y_val.mean():.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save the files so we can re use them in the model's notebook\n",
    "train_features.to_parquet(\"train_features.parquet\")\n",
    "test_features.to_parquet(\"test_features.parquet\")\n",
    "pd.DataFrame({\"feature\": feature_columns}).to_parquet(\"feature_columns.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
